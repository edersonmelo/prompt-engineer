{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "288ce9a8",
      "metadata": {
        "id": "288ce9a8"
      },
      "source": [
        "# LangChain Basics\n",
        "\n",
        "Last Update: Jan 10, 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "w__KJC3d2bn0",
        "outputId": "40c6f3d2-976c-4273-b340-791b1c1558de"
      },
      "id": "w__KJC3d2bn0",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-160b0ea4-4777-47ea-9090-bd44d67287b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-160b0ea4-4777-47ea-9090-bd44d67287b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements.txt to requirements (4).txt\n",
            "User uploaded file \"requirements (4).txt\" with length 116 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "002d92f1",
      "metadata": {
        "id": "002d92f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b50fe59-e0e4-40e0-ba70-952dac95a7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `pip install -r ./requirements (4).txt -q'\n"
          ]
        }
      ],
      "source": [
        "pip install -r ./requirements (4).txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XmKV1Drq2cZu"
      },
      "id": "XmKV1Drq2cZu"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DkUwd_3g2c4r"
      },
      "id": "DkUwd_3g2c4r"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LoSi7pnI2dJ0"
      },
      "id": "LoSi7pnI2dJ0",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d7f8a230",
      "metadata": {
        "id": "d7f8a230"
      },
      "source": [
        "Download [requirements.txt](\n",
        "https://drive.google.com/file/d/1IeTp3JOjhkHYr21tEEh_7X8ozx39v2xc/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4ff28727",
      "metadata": {
        "id": "4ff28727"
      },
      "outputs": [],
      "source": [
        "# !pip - installs the packages in the base environment\n",
        "# pip - installs the packages in the virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bdf2f431",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdf2f431",
        "outputId": "763a3c99-dbdd-4a0e-e9a1-dd0361b03158",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: langchain\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1d3d52dc",
      "metadata": {
        "id": "1d3d52dc",
        "outputId": "bce19c3d-9e9a-4459-e25d-591ac10c07e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: openai\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d14256a6",
      "metadata": {
        "id": "d14256a6"
      },
      "outputs": [],
      "source": [
        "# upgrading langchain\n",
        "# pip install langchain --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4407a3a",
      "metadata": {
        "id": "f4407a3a"
      },
      "source": [
        "#### Python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d9420415",
      "metadata": {
        "id": "d9420415",
        "outputId": "ad1e35e3-44f9-4130-80d1-17293ef2970f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e9abd3978d34>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loading the API Keys (OpenAI, Pinecone) from .env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# loading the API Keys (OpenAI, Pinecone) from .env\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "# os.environ.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20ee709",
      "metadata": {
        "id": "c20ee709"
      },
      "source": [
        "### ChatModels: GPT-3.5-Turbo and GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ce9d7e",
      "metadata": {
        "id": "f8ce9d7e"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='langchain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054eadaa",
      "metadata": {
        "id": "054eadaa"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import(\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9bdecaa",
      "metadata": {
        "id": "f9bdecaa"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
        "messages = [\n",
        "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
        "    HumanMessage(content='explain quantum mechanics in one sentence')\n",
        "]\n",
        "output = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761e06dd",
      "metadata": {
        "id": "761e06dd",
        "outputId": "38f630b1-14d4-4fd5-d39b-70e5f0072d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantenmechanik beschreibt das Verhalten von Teilchen auf subatomarer Ebene durch Wahrscheinlichkeiten und Superpositionszustände.\n"
          ]
        }
      ],
      "source": [
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d3ad29b",
      "metadata": {
        "id": "6d3ad29b"
      },
      "source": [
        "### Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a31ca4b",
      "metadata": {
        "id": "3a31ca4b"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612ed8a8",
      "metadata": {
        "id": "612ed8a8",
        "outputId": "7cf93377-faea-47c8-a86e-d23de5591459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['language', 'virus'] template='You are an experienced virologist.\\nWrite a few sentences about the following {virus} in {language}.'\n"
          ]
        }
      ],
      "source": [
        "template = '''You are an experienced virologist.\n",
        "Write a few sentences about the following {virus} in {language}.'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['virus', 'language'],\n",
        "    template=template\n",
        ")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28accc9",
      "metadata": {
        "id": "b28accc9",
        "outputId": "9a33f6f3-5b1c-4688-b5c9-51f1419ce176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HIV ist ein Virus, das das Immunsystem des menschlichen Körpers angreift und zu AIDS führen kann. HIV wird hauptsächlich durch ungeschützten Geschlechtsverkehr, gemeinsame Nutzung von infizierten Nadeln oder von einer infizierten Mutter auf ihr Kind während der Schwangerschaft, Geburt oder Stillzeit übertragen. Es gibt verschiedene Arten von HIV, aber die häufigste ist HIV-1. Obwohl es keine Heilung für HIV gibt, können antiretrovirale Medikamente das Virus kontrollieren und das Fortschreiten zu AIDS verlangsamen.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = OpenAI(model_name='gpt-3.5-turbo', temperature=0.7)\n",
        "output = llm(prompt.format(virus='HIV', language='German'))\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025d9b84",
      "metadata": {
        "id": "025d9b84"
      },
      "source": [
        "### Simple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1e5278",
      "metadata": {
        "id": "fd1e5278"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
        "template = '''You are an experienced virologist.\n",
        "Write a few sentences about the following {virus} in {language}.'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['virus', 'language'],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "output = chain.run({'virus': 'HSV', 'language': 'french'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be42818a",
      "metadata": {
        "id": "be42818a",
        "outputId": "af68df04-3165-4c7c-ef0e-f6a60d03dd28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L'HSV, ou le virus de l'herpès simplex, est un virus très répandu qui infecte les humains. Il existe deux types d'HSV : l'HSV-1, qui provoque généralement des éruptions buccales, et l'HSV-2, qui est principalement responsable des infections génitales. Ces virus sont transmis par contact direct avec les lésions cutanées ou les muqueuses infectées. Bien qu'il n'existe pas de traitement curatif pour l'HSV, des médicaments antiviraux peuvent aider à réduire les symptômes et les récidives. Il est important de prendre des précautions pour prévenir la transmission de l'HSV, notamment en évitant les contacts intimes lors d'éruptions actives.\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f283cc13",
      "metadata": {
        "id": "f283cc13"
      },
      "source": [
        "### Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d68115d",
      "metadata": {
        "id": "9d68115d",
        "outputId": "2aa75fcc-88bb-4972-c9dc-0eec0f0b7d07",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mSure! Here's a function that implements the concept of softmax in Python:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "\n",
            "def softmax(x):\n",
            "    \"\"\"\n",
            "    Compute the softmax of a given array of numbers.\n",
            "    \n",
            "    Arguments:\n",
            "    x -- A numpy array of shape (n,) containing the input values.\n",
            "    \n",
            "    Returns:\n",
            "    softmax_vals -- A numpy array of shape (n,) containing the softmax values.\n",
            "    \"\"\"\n",
            "    \n",
            "    # Ensure the input is a numpy array\n",
            "    x = np.array(x)\n",
            "    \n",
            "    # Compute the exponential of each element in the input array\n",
            "    exp_vals = np.exp(x)\n",
            "    \n",
            "    # Compute the sum of exponential values\n",
            "    sum_exp = np.sum(exp_vals)\n",
            "    \n",
            "    # Compute the softmax values by dividing each exponential value by the sum of exponential values\n",
            "    softmax_vals = exp_vals / sum_exp\n",
            "    \n",
            "    return softmax_vals\n",
            "```\n",
            "\n",
            "You can use this function by passing in an array of numbers as input. The function will return an array of the same shape, where each element represents the softmax value for the corresponding input element.\n",
            "\n",
            "Here's an example usage:\n",
            "\n",
            "```python\n",
            "x = [1, 2, 3, 4, 5]\n",
            "softmax_vals = softmax(x)\n",
            "print(softmax_vals)\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n",
            "```\n",
            "\n",
            "The softmax function is commonly used in machine learning for tasks such as multiclass classification, where it converts a vector of arbitrary real values into a probability distribution over different classes.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mThe softmax function is a mathematical function that converts an input array of real values into a probability distribution. It is commonly used in machine learning algorithms for multiclass classification tasks, where it predicts the probability of each class being the correct answer.\n",
            "\n",
            "The softmax function takes an input array of arbitrary real values and computes the exponential of each element. It then computes the sum of these exponential values. Finally, it divides each exponential value by the sum of all exponential values to obtain the softmax values.\n",
            "\n",
            "The softmax function is useful because it scales the input values to ensure that they fall within the range of [0, 1] and sum up to 1. This allows the output values to be interpreted as probabilities. The softmax values represent the confidence or likelihood of each class being the correct prediction.\n",
            "\n",
            "In the provided Python code, the softmax function is implemented using the NumPy library. First, the input array is converted into a NumPy array to ensure compatibility with NumPy operations. Then, the exponential values are computed using `np.exp()`. The sum of exponential values is obtained with `np.sum()`. Finally, the softmax values are computed by dividing each exponential value by the sum of exponential values. The resulting softmax values are returned as a NumPy array.\n",
            "\n",
            "To use the softmax function, you simply need to pass an array of numbers as input. The function will return an array of the same shape as the input, where each element represents the softmax value for the corresponding input element.\n",
            "\n",
            "In the example usage provided, an input array `x = [1, 2, 3, 4, 5]` is passed to the softmax function. The function computes the softmax values, and the result `[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]` is printed. These values represent the probabilities of each class being the correct prediction based on the input values.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.7, max_tokens=1024)\n",
        "prompt1 = PromptTemplate(\n",
        "    input_variables=['concept'],\n",
        "    template='''You are an experienced scientist and Python programmer.\n",
        "    Write a function that implements the concept of {concept}.'''\n",
        ")\n",
        "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
        "\n",
        "\n",
        "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=['function'],\n",
        "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
        ")\n",
        "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
        "output = overall_chain.run('softmax')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3a9c39",
      "metadata": {
        "id": "ed3a9c39"
      },
      "source": [
        "### LangChain Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8284aa",
      "metadata": {
        "id": "3f8284aa",
        "outputId": "38ec24c4-f561-47bf-fbf3-235999fec2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631fd9cf",
      "metadata": {
        "id": "631fd9cf"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115e2fe3",
      "metadata": {
        "id": "115e2fe3",
        "outputId": "e7b894b6-3eee-41b4-faf3-171d2aed2f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo calculate the square root of the factorial of 12, we need to find the factorial of 12 first and then take the square root of the result. We can use the math module in Python to calculate the square root. To display the result with 4 decimal points, we can use the format function or the round function.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: import math\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have imported the math module, which will allow me to use the square root function.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: math.factorial(12)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have calculated the factorial of 12, which is 479,001,600.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: math.sqrt(479001600)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have calculated the square root of the factorial of 12, which is approximately 21,908.9023.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: round(math.sqrt(479001600), 4)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have rounded the square root of the factorial of 12 to 4 decimal points, which is 21,908.9023.\n",
            "\n",
            "Final Answer: 21,908.9023\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'21,908.9023'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "agent_executor = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "agent_executor.run('Calculate the square root of the factorial of 12 and display it with 4 decimal points')\n",
        "\n",
        "# agent_executor.run('What is the answer to 5.1 ** 7? \\\n",
        "# Generate the Python code for this and run the code.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0af1eb",
      "metadata": {
        "id": "8f0af1eb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}